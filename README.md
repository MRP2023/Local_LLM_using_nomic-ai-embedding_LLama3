# Local_LLM_using_nomic-ai-embedding_LLama3
ğŸ§  A Streamlit-based AI assistant that lets you upload PDFs, extract tables and text (with OCR fallback), and chat with your documents using LLMs + vector search via ChromaDB and Nomic embeddings.

# ğŸ“„ Chat with Your PDF â€“ AI-powered Q&A on Documents

This project allows you to upload PDF files (with structured text or scanned tables), extract their content using `pdfplumber`, `pandas`, and OCR (via `pytesseract`), and then ask questions directly about the document. Answers are powered by vector-based retrieval (using [ChromaDB](https://www.trychroma.com/)) and an LLM backend (e.g., LLaMA3 via Ollama).

---

## ğŸ”§ Features

- ğŸ“ Upload PDFs with plain text, scanned text, or tabular data
- ğŸ§  Automatic text and table extraction using `pdfplumber` + `pandas`
- ğŸ–¼ï¸ OCR fallback for image-based PDFs (via `pytesseract`)
- ğŸ¤– Ask natural language questions about the PDF
- ğŸ§© Nomic Embeddings + ChromaDB vector storage
- ğŸ§¼ Reset knowledge base with a single click
- ğŸ“‹ Logs extracted table headers for debugging

---

## ğŸ“¦ Stack

- [Streamlit](https://streamlit.io/) â€“ UI framework
- [LangChain](https://www.langchain.com/) â€“ RetrievalQA, chaining
- [ChromaDB](https://www.trychroma.com/) â€“ Vector database
- [Ollama](https://ollama.com/) â€“ Local LLMs like LLaMA 3
- [Nomic Embeddings](https://docs.nomic.ai/) â€“ Open-source text embeddings
- `pandas`, `pdfplumber`, `pytesseract`, `pdf2image` â€“ For PDF and table processing

---

## ğŸš€ Quick Start

```bash
# Clone the repo
git@github.com:MRP2023/Local_LLM_using_nomic-ai-embedding_LLama3.git

# Install dependencies
pip install -r requirements.txt

# (Make sure Docker-based ChromaDB is running)
# e.g.:
# docker run -v ./chroma-data:/data -p 8000:8000 chromadb/chroma

# Start the app
streamlit run app_ocr_table_using_panda.py


#Additional System Requirements:
#1. ChromaDB (Vector Database)
#Run ChromaDB using Docker:
docker run -d -v ./chroma-data:/data -p 8000:8000 chromadb/chroma
#To reset:
rm -rf ./chroma-data/*


```
2. Ollama (Local LLM Backend)
Install: https://ollama.com

3. Tesseract OCR (for scanned PDFs)
```
#Ubuntu
sudo apt install tesseract-ocr

#macOS:
brew install tesseract

#Windows:
https://github.com/tesseract-ocr/tesseract

#Verify:
tesseract --version

```
Summary Table:
Component      | Used For                         | Setup Command
---------------|----------------------------------|------------------------------
ChromaDB       | Vector DB for embeddings         | docker run -v ...
Ollama         | Local LLM (e.g., LLaMA 3)        | ollama run llama3
Tesseract OCR  | Image-based PDF text extraction  | apt/brew install tesseract






